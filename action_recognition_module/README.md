# Object-PALM: Exploring VLMs for Object-Centric Action Recognition and Forecasting
## Action Recognition Module

The action recognition module trains a model that is used to predict an action label ((verb,noun) pair) for the action segments of the Ego4d dataset. It contains three building blocks.
1. [EgoVLP](EgoVLP/) is used to generate video features for each of the action segments.
   - Refer to official github page (https://github.com/showlab/EgoVLP/tree/main) to download pretrained model (EgoVLP_PT_BEST: egovlp.pth).
   - Save data features with the following line
   ```python run/test_lta.py --gpu 1 --config configs/eval/lta.json --save_feats /your/save/directory/ --split train```.
   You need to adjust the split and the directories specified in the [config file](EgoVLP/configs/eval/lta.json) related to the model and the data directories. You can also use the [EGOVLP_run.sh](EgoVLP/EGOVLP_run.sh) script. The elements installed in the environment used can be found [here](EgoVLP/requirements_egovlp_venv.txt). 
   - The final action recognition model also uses the embeddings obtained after applying EgoVLP text encoder to the nouns of the Ego4d taxonomy. Use `python get_egovlp_noun_embeddings.py` or `bash noun_embeds.sh` to extract them. Remember to adjust the paths inside the script.

2. Use the [object recognition module](object-recognition) to obtain object recognition results. For each action segment, a probability score between 0 and 1 is computed for all the nouns present in the Ego4d taxonomy. This score measures the probability of the object appearing close to where the action is being performed during the action segment. Due to its significance, this module is explained separately in [its directory](object-recognition).

3. Use the outputs of the previous modules to train our action recognition model at [recognition-model](recognition-model/forecasting-main/).
   - This model was developed using the code from PALM as a foundation, which itself was built on the code from [Ego4D_LTA](https://github.com/EGO4D/forecasting/blob/main/LONG_TERM_ANTICIPATION.md). However, this code was unnecessary complex for this task. Thus, in this repository we have a simplified version that does not include some files related to other tasks or other type of models. Anyway, if necessary, this model could be easily included in the previous codebase by including the `ActionRecognitionModel` defined in [lta_models.py](recognition-model/forecasting-main/ego4d_forecasting/models/lta_models.py) and the dataset modifications related to the object recognition module.
   - We use checkpoint model ```pretrained_models/long_term_anticipation/lta_slowfast_trf.ckpt``` from [Ego4D_LTA](https://github.com/EGO4D/forecasting/blob/main/LONG_TERM_ANTICIPATION.md) to initialize the transformer network.
   - This code is prepared for running several experiments depending on the postprocessing done to the outputs of the [object recognition module](object-recognition). To select the particular experiment, adjust the necessary variables at the top of [ptv_dataset_helper.py](recognition-model/forecasting-main/ego4d_forecasting/datasets/ptv_dataset_helper.py).
   - Remember to also adjust the path variables in [ego4d_train_ar.sh](recognition-model/forecasting-main/tools/action_recognition/ego4d_train_ar.sh), [MULTISLOWFAST_8x8_R101.yaml](recognition-model/forecasting-main/tools/action_recognition/ego4d_val_ar.sh), and [ego4d_train_ar.sh](recognition-model/forecasting-main/configs/Ego4dLTA/MULTISLOWFAST_8x8_R101.yaml).
   - Train the model using [train_model.sh](recognition-model/forecasting-main/train_model.sh) and evaluate it on the validation set using [evaluate_model.sh](recognition-model/forecasting-main/evaluate_model.sh). The evaluation saves the predictions in a file named `outputs_test_step.json`. The environment used in those scripts had installed the libraries specified in [requirements_ar_venv.txt](recognition-model/requirements_ar_venv.txt).
   - One of the experiment options uses a synthetic input instead of the output of the object recognition module. The files specifying those inputs ([poc_10_nouns_6_past_3_rand_no_rep_train.json](recognition-model/poc_10_nouns_6_past_3_rand_no_rep_train.json) and [poc_10_nouns_6_past_3_rand_no_rep_val.json](recognition-model/poc_10_nouns_6_past_3_rand_no_rep_val.json)) were generated using [select_nouns_based_on_past.py](recognition-model/select_nouns_based_on_past.py).